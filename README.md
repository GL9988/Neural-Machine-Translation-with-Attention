# ğŸŒ Neural Machine Translation with Attention  
Custom GRU-based sequence-to-sequence model with attention for Englishâ€“French translation.  

---

## ğŸš€ Motivation  
Built to explore language modeling, attention mechanisms, and sequence learning from scratchâ€”balancing technical rigor with team collaboration.

---

## ğŸ“¦ Dataset  
Sourced from the Tatoeba Project via the "fra.txt" bilingual dataset (18K+ pairs).  
Filtered for clean, short, prefix-based sentence pairs to optimize alignment and fluency.

---

## ğŸ§  Model Architectures  
- **Encoder**: GRU with embedding and dropout  
- **Decoder**: GRU with additive attention and teacher forcing  
- **Tokenization**: Custom vocabulary class with START/END tokens  

---

## ğŸ§ª Evaluation  
| Method              | BLEU Score | Notes                     |
|---------------------|------------|---------------------------|
| GRU + Attention     | ~0.96      | Consistent across pairs   |
| Visual Output       | âœ…         | Alignment heatmaps shown  |

---

## ğŸ“Š Visualization  
<p align="center">
  <img src="visualizations/attention_heatmap_sample.png" width="480"/>
  <br><em>Attention heatmap: French input vs. English output</em>
</p>

---

## ğŸ§° Tech Stack  
- **Language**: Python  
- **Framework**: PyTorch  
- **Tools**: NumPy, Matplotlib, Seaborn  
- **Evaluation**: BLEU Score, Attention Maps

---

## ğŸ¤ Collaboration  
Led problem scoping, built training loop, and handled model debugging.  
Worked closely with teammates to align logic, resolve tensor mismatches, and integrate modules.

---

## â–¶ï¸ Run This Project  
```bash
git clone https://github.com/yourusername/nmt-gru-attention.git
cd nmt-gru-attention
pip install -r requirements.txt
python train_model.py
